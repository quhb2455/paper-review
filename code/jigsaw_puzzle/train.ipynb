{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f1e5bf0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import albumentations as A\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from itertools import permutations\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from easydict  import EasyDict "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec7fd90",
   "metadata": {},
   "source": [
    "## Model - AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "773ec7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module) :\n",
    "    def __init__(self, \n",
    "                 first_in=3, \n",
    "                 first_stride=4, \n",
    "                 dropout_rate=0.5, \n",
    "                 in_feature=256*6*6,\n",
    "                 out_feature=4608,\n",
    "                 num_classes=1000\n",
    "                ) :\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=first_in, \n",
    "                      out_channels=96, \n",
    "                      kernel_size=11, \n",
    "                      padding=2, \n",
    "                      stride=first_stride),\n",
    "            nn.ReLU(),\n",
    "            nn.LocalResponseNorm(size=5, \n",
    "                                 k=2,    \n",
    "                                 alpha=0.0001, \n",
    "                                 beta=0.75),\n",
    "            nn.MaxPool2d(kernel_size=3, \n",
    "                        stride=2),\n",
    "        )\n",
    "        \n",
    "        self.conv_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=96, \n",
    "                     out_channels=256,\n",
    "                      padding=2,\n",
    "                      kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.LocalResponseNorm(size=5, \n",
    "                                 k=2,    \n",
    "                                 alpha=0.0001, \n",
    "                                 beta=0.75),\n",
    "            nn.MaxPool2d(kernel_size=3, \n",
    "                        stride=2),\n",
    "        )\n",
    "        \n",
    "        self.conv_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, \n",
    "                     out_channels=384,\n",
    "                      padding=1,\n",
    "                      kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.conv_4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=384, \n",
    "                     out_channels=384,\n",
    "                      padding=1,\n",
    "                      kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.conv_5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=384, \n",
    "                     out_channels=256,\n",
    "                      padding=1,\n",
    "                      kernel_size=3,\n",
    "                     ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, \n",
    "                         stride=2),\n",
    "        )\n",
    "                \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            self.conv_1, \n",
    "            self.conv_2, \n",
    "            self.conv_3, \n",
    "            self.conv_4, \n",
    "            self.conv_5, \n",
    "        )\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(in_feature, out_feature),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(out_feature, out_feature),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(out_feature, num_classes),\n",
    "        )\n",
    "\n",
    "        \n",
    "        for idx, layers in enumerate(self.conv_layers):\n",
    "            self.initialization(idx, layers)\n",
    "\n",
    "        self.initialization('fc', self.fc_layers)\n",
    "\n",
    "    def initialization(self, idx, layers) :\n",
    "        for layer in layers :\n",
    "            if 'weight' in dir(layer):\n",
    "                nn.init.normal_(layer.weight, mean=0, std=0.01)                \n",
    "                if idx in ['fc', 1, 3, 4] :\n",
    "                    nn.init.constant_(layer.bias.data, 1)\n",
    "                elif idx in [0, 2] :\n",
    "                    nn.init.constant_(layer.bias.data, 0)    \n",
    "                    \n",
    "    def forward(self, x) :\n",
    "        x = self.conv_layers(x)\n",
    "        # x = x.contiguous().view(-1) \n",
    "\n",
    "        x = x.flatten(1)\n",
    "\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce37eab",
   "metadata": {},
   "source": [
    "## Model - CFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "11577b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFN(nn.Module) :\n",
    "    def __init__(self, \n",
    "                 in_channel=27, \n",
    "                 strd=2, \n",
    "                 in_feature=256*3*3, \n",
    "                 out_feature=4608, \n",
    "                 num_classes=69) :\n",
    "        super(CFN, self).__init__()\n",
    "        self.alexnet = AlexNet(first_in=in_channel, \n",
    "                               first_stride=strd, \n",
    "                               in_feature=in_feature, \n",
    "                               out_feature=out_feature\n",
    "                              )\n",
    "        # alexnet 논문에 나와있는 방법으로 초기화한 layer들을 가져옴\n",
    "        self.conv_layers = self.alexnet.conv_layers\n",
    "        self.fc6 = self.alexnet.fc_layers[0]\n",
    "        \n",
    "        # fc7, fc8, output 포함\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(out_feature, 4096), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 100), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.alexnet.initialization('fc', self.classifier)\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.fc6(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c7dbfd",
   "metadata": {},
   "source": [
    "## puzzle pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7ffe7502",
   "metadata": {},
   "outputs": [],
   "source": [
    "class jigsaw_pipeline() :\n",
    "    def __init__(self, normalize=True, color_jitter=False) :\n",
    "        self.color_jitter = color_jitter\n",
    "        \n",
    "        if normalize :\n",
    "            self.transforms = A.Compose([\n",
    "                A.RandomCrop(64, 64),\n",
    "                A.Normalize()\n",
    "            ])\n",
    "        else : \n",
    "            self.transforms = A.Compose([\n",
    "                A.RandomCrop(64, 64)\n",
    "            ])\n",
    "            \n",
    "    def __call__(self, img) :\n",
    "        h, w, c = img.shape\n",
    "        \n",
    "        # step 1. resize width or height to 256 with preserve the original aspect ratio.\n",
    "        if h < w :\n",
    "            resized_w = int(w / h * 256)\n",
    "            img = cv2.resize(img, (256 , resized_w))\n",
    "\n",
    "        elif w <= h :\n",
    "            resized_h = int(h / w * 256)\n",
    "            img = cv2.resize(img, (resized_h, 256))\n",
    "        \n",
    "        # step 2. Random crop size 225 x 225\n",
    "        img = A.RandomCrop(225, 225)(image = img)['image']\n",
    "        \n",
    "        # step 3 and 4. split 3 x 3 grid of 75 x 75 pixels tiles and random crop 64 x 64\n",
    "        for i in range(3) :\n",
    "            for j in range(3) :\n",
    "                crop_img = img[i*75 : (i * 75) + 75, j*75 : (j * 75) + 75, :]\n",
    "                \n",
    "                if i == 0 and j == 0 :    \n",
    "                    tile_img = self.transforms(image = crop_img)['image']\n",
    "                else : \n",
    "                    tile_img = np.concatenate((tile_img,self.transforms(image = crop_img)['image']),axis=2)\n",
    "        \n",
    "        return tile_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a24462",
   "metadata": {},
   "source": [
    "## Generate Permutation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "54714ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_permutation_sets(number_permu_set=100, max_hamming=True, save=True) :\n",
    "\n",
    "    item = [i for i in range(1,10)]\n",
    "    permutate_items = np.array(list(permutations(item, 9)))\n",
    "    max_hamming = True\n",
    "\n",
    "    N = number_permu_set # permutation 개수\n",
    "    j = np.random.choice(len(permutate_items), 1, replace=False)\n",
    "\n",
    "    for i in tqdm(range(1, N+1)) :\n",
    "        if i == 1 :\n",
    "            p = np.array(permutate_items[j])\n",
    "        else : \n",
    "            hat_p = np.array(permutate_items[j]).reshape([1, -1])\n",
    "            p = np.concatenate([p, hat_p], axis=0)\n",
    "\n",
    "        permutate_items = np.delete(permutate_items, j, axis=0)\n",
    "        d = distance.cdist(p, permutate_items, metric='hamming').mean(axis=0)\n",
    "\n",
    "        if max_hamming == True :\n",
    "            j = np.argmax(d)\n",
    "        else : \n",
    "            j = np.argmin(d)\n",
    "            \n",
    "    if save :\n",
    "        np.save(f'permutation_{N}_sets.npy', p)\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d81aa9",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b6af1aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JIGSAW_DATASET(Dataset) :\n",
    "    def __init__(self, img_list, permu_set, normalize=True) :\n",
    "        self.img_list = img_list\n",
    "        self.permu_set = permu_set\n",
    "        self.pipeline = jigsaw_pipeline(normalize=normalize)\n",
    "    \n",
    "    def __len__(self) :\n",
    "        return len(self.img_list)\n",
    "    \n",
    "    def shuffle_tiles(self, tile, permu):\n",
    "        shuffled_tile = np.zeros_like(tile)\n",
    "        for idx, num in enumerate(permu, start=1) :\n",
    "            shuffled_tile[:, :, (idx-1)*3 : idx*3] = tile[:, :, (num-1) * 3 : num * 3]\n",
    "        return shuffled_tile\n",
    "            \n",
    "    def __getitem__(self, idx) :\n",
    "        img = cv2.imread(self.img_list[idx])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = self.pipeline(img)\n",
    "        \n",
    "        permu_idx = np.random.randint(self.permu_set.shape[0])        \n",
    "        permu = self.permu_set[permu_idx]\n",
    "        \n",
    "        img = self.shuffle_tiles(img, permu)\n",
    "        img = img.transpose(2,0,1)\n",
    "\n",
    "        return torch.FloatTensor(img), torch.tensor(permu_idx, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104f42d7",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "03739554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _accuracy(pred, label) :\n",
    "    idx_pred = torch.argmax(pred, dim=1)\n",
    "    acc = idx_pred == label\n",
    "    return acc.sum() / acc.shape[0]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c83cc84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(args) :\n",
    "    model = CFN(in_feature=1024, num_classes=args.num_classes).to(args.device)\n",
    "    optimizer = torch.optim.SGD(params=model.parameters(), lr=args.learning_rate)\n",
    "    criterion  = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    ps = np.load(args.permutation_path)\n",
    "    img_list = glob(args.img_path)\n",
    "    \n",
    "    jigsaw_dataset = JIGSAW_DATASET(img_list, ps, normalize=False)\n",
    "    dataloader = DataLoader(jigsaw_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "    for E in range(1, args.epochs+1) :\n",
    "        avg_loss, avg_acc = 0, 0\n",
    "        for img, label in dataloader:\n",
    "            model.train()\n",
    "            img = img.to(args.device)\n",
    "            label = label.to(args.device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(img)\n",
    "            loss = criterion(pred, label)\n",
    "            acc = _accuracy(pred.cpu(), label.cpu())\n",
    "            \n",
    "            avg_loss += loss.cpu().item()\n",
    "            avg_acc += acc.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"Epochs :\", E, f\" || ACC : {avg_acc/len(dataloader):0.4f}  ||  Loss : {avg_loss/len(dataloader):0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef68815",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c7069c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "Options = {\n",
    "    'permutation_path' : './permutation_30_sets.npy',\n",
    "    'batch_size' : 64,\n",
    "    'epochs' : 70,\n",
    "    'learning_rate' : 0.01,\n",
    "    'device' : \"cuda:0\",\n",
    "    \"img_path\" : './data/*',\n",
    "    \"num_classes\" : 30 # should be same as number of permutations set\n",
    "}\n",
    "args = EasyDict(Options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "411d1546",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs : 1  || ACC : 0.0384  ||  Loss : 3.3975\n",
      "Epochs : 2  || ACC : 0.0156  ||  Loss : 3.4285\n",
      "Epochs : 3  || ACC : 0.0192  ||  Loss : 3.4104\n",
      "Epochs : 4  || ACC : 0.0348  ||  Loss : 3.4112\n",
      "Epochs : 5  || ACC : 0.0365  ||  Loss : 3.4155\n",
      "Epochs : 6  || ACC : 0.0104  ||  Loss : 3.4069\n",
      "Epochs : 7  || ACC : 0.0244  ||  Loss : 3.3987\n",
      "Epochs : 8  || ACC : 0.0104  ||  Loss : 3.4188\n",
      "Epochs : 9  || ACC : 0.0365  ||  Loss : 3.3963\n",
      "Epochs : 10  || ACC : 0.0367  ||  Loss : 3.4115\n",
      "Epochs : 11  || ACC : 0.0244  ||  Loss : 3.4127\n",
      "Epochs : 12  || ACC : 0.0156  ||  Loss : 3.4183\n",
      "Epochs : 13  || ACC : 0.0156  ||  Loss : 3.4075\n",
      "Epochs : 14  || ACC : 0.0260  ||  Loss : 3.4077\n",
      "Epochs : 15  || ACC : 0.0296  ||  Loss : 3.4066\n",
      "Epochs : 16  || ACC : 0.0260  ||  Loss : 3.3981\n",
      "Epochs : 17  || ACC : 0.0384  ||  Loss : 3.4013\n",
      "Epochs : 18  || ACC : 0.0365  ||  Loss : 3.4015\n",
      "Epochs : 19  || ACC : 0.0348  ||  Loss : 3.4037\n",
      "Epochs : 20  || ACC : 0.0419  ||  Loss : 3.4066\n",
      "Epochs : 21  || ACC : 0.0296  ||  Loss : 3.4136\n",
      "Epochs : 22  || ACC : 0.0156  ||  Loss : 3.4160\n",
      "Epochs : 23  || ACC : 0.0507  ||  Loss : 3.3940\n",
      "Epochs : 24  || ACC : 0.0104  ||  Loss : 3.4085\n",
      "Epochs : 25  || ACC : 0.0208  ||  Loss : 3.4085\n",
      "Epochs : 26  || ACC : 0.0208  ||  Loss : 3.4033\n",
      "Epochs : 27  || ACC : 0.0417  ||  Loss : 3.3988\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [78]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [76]\u001b[0m, in \u001b[0;36mtraining\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m E \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, args\u001b[38;5;241m.\u001b[39mepochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) :\n\u001b[0;32m     13\u001b[0m     avg_loss, avg_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m img, label \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m     15\u001b[0m         model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     16\u001b[0m         img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mto(args\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch-1.11\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch-1.11\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch-1.11\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch-1.11\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Input \u001b[1;32mIn [70]\u001b[0m, in \u001b[0;36mJIGSAW_DATASET.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx) :\n\u001b[1;32m---> 17\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m     19\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline(img)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e957ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45ef587c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3,4,5])\n",
    "b = torch.tensor([2,2,3,4,5])\n",
    "\n",
    "c = a == b\n",
    "c.sum()/c.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c25966dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0699ec7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
